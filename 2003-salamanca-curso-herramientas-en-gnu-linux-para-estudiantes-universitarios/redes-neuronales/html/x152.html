<HTML
><HEAD
><TITLE
>Redes Autoorganizadas. Redes SOFM</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.7"><LINK
REL="HOME"
TITLE="Herramientas en GNU/Linux para estudiantes universitarios"
HREF="index.html"><LINK
REL="UP"
TITLE="Tipos de Redes Neuronales"
HREF="c81.html"><LINK
REL="PREVIOUS"
TITLE="El perceptron multicapa (MLP)"
HREF="x105.html"><LINK
REL="NEXT"
TITLE="Redes de funcion de base radial (RBF)"
HREF="x185.html"><LINK
REL="stylesheet"
HREF="./base.css"
TYPE="text/css"></HEAD
><BODY
CLASS="SECT1"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>Herramientas en GNU/Linux para estudiantes universitarios: </TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="x105.html"
ACCESSKEY="P"
>Anterior</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
>Capítulo 3. Tipos de Redes Neuronales</TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
><A
HREF="x185.html"
ACCESSKEY="N"
>Siguiente</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="AEN152"
></A
>3.3. Redes Autoorganizadas. Redes SOFM</H1
><P
>En este tipo de redes el entrenamiento o aprendizaje es diferente al de las redes con
entrenamiento supervisado. A la red no se le suministra junto a los patrones de
entrenamiento, una salida deseada. Lo que hará la red es encontrar regularidades o
clases en los datos de entrada, y modificar sus pesos para ser capaz de reconocer
estas regularidades o clases. 
       </P
><P
>Uno de los tipos de redes que pertenece a esta familia y que se ha usado bastante son
los mapas autoorganizados, SOM (Self-Organizing Maps). La arquitectura típica de este
tipo de mapas es la siguiente:
       <DIV
CLASS="FIGURE"
><A
NAME="AEN156"
></A
><P
><B
>Figura 3-4. Arquitectura típica de un mapa SOM</B
></P
><P
><IMG
SRC="som.jpg"
ALIGN="CENTER"></P
></DIV
>
       </P
><P
>Como se puede apreciar es una red de tipo unidireccional. La red se organiza en dos
capas, siendo la primera capa la formada por las neuronas de entrada. La segunda capa
consiste en un array de neuronas de dos dimensiones. Como se necesitan dos índices para
etiquetar cada neurona, los pesos sinapticos asociados a cada neurona tendran tres
índices (i,j,k) donde (i,j) indican la posición de la neurona en la capa y k, la
componente o conexión con cierta neurona de entrada.
       </P
><P
>En cuanto al entrenamiento, este es un ejemplo de red que utiliza un aprendizaje de tipo
no supervisado. Además, cada neurona utiliza como regla de propagacion una distancia de
su vector de pesos sinápticos al patrón de entrada. Otros conceptos importantes que
intervienen en el proceso de aprendizaje son los conceptos de neurona ganadora y
vecindad de la misma. Un algoritmo de aprendizaje muy usado con este tipo de redes es
el algoritmo de Kohonen que se describe como sigue:       
       </P
><P
><P
></P
><OL
TYPE="1"
><LI
><P
>Inicializacion de los pesos w<SUB
>ijk</SUB
>. Hay varias opciones posibles
    </P
></LI
><LI
><P
>Eleccion de un patrón de entre el conjunto de patrones de entrenamiento
    </P
></LI
><LI
><P
>Para cada neurona del mapa, calcular la distancia euclídea entre el patrón de entrada
<B
CLASS="KEYCAP"
>x</B
> y el vector de pesos sinápticos 
 <DIV
CLASS="EQUATION"
><A
NAME="AEN171"
></A
><P
><B
>Ecuación 3-3. Distancia Euclídea entre el vector sináptico y la entrada</B
></P
><IMG
SRC="distancia.jpg"
ALIGN="CENTER"></DIV
>
    </P
></LI
><LI
><P
>Evaluar la neurona ganadora, es decir aquella cuya distancia es la menor de todas
    </P
></LI
><LI
><P
>Actualizar los pesos sinápticos de la neurona ganadora y de sus vecinas según la regla:
 <DIV
CLASS="EQUATION"
><A
NAME="AEN179"
></A
><P
><B
>Ecuación 3-4. Actualización de los pesos en una red SOM</B
></P
><IMG
SRC="pesossom.jpg"
ALIGN="CENTER"></DIV
>
alfa(t) es un factor llamado ritmo de aprendizaje que da cuenta de la importancia que la
diferencia entre el patrán y los pesos tiene en el ajuste de los mismos a lo largo del
proceso de aprendizaje. Hay varias posibilidades para esta función, desde un constante
hasta algún tipo de funcion monótona decreciente con el tiempo. h es una función de
vecindad que nos indica en que medida se modifican los pesos de las neuronas vecinas.
Con esto quiere decir que cuando la neurona ganadora modifica sus pesos, la vecindad de
esta neurona lo hace también, en mayor o menor medida según sea la forma funcional de h.
En general, las funciones empleadas para h tienen un máximo en |i-j|=0 y decrecen más o
menos rapido a medida que esta distancia aumenta.
    </P
></LI
><LI
><P
>Lo usual es fijar un numero de iteraciones antes de comenzar el aprendizaje. Si no se
llegó al numero de iteraciones establecido previamente, se vuelve al paso 2. Sobre este
número de iteraciones necesario, se suelen tomar criterios como el número de neuronas en
el mapa.
    </P
></LI
></OL
>
       </P
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="x105.html"
ACCESSKEY="P"
>Anterior</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
ACCESSKEY="H"
>Inicio</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="x185.html"
ACCESSKEY="N"
>Siguiente</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>El perceptron multicapa (MLP)</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="c81.html"
ACCESSKEY="U"
>Subir</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>Redes de funcion de base radial (RBF)</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>