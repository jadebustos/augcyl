<HTML
><HEAD
><TITLE
>El perceptron multicapa (MLP)</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.7"><LINK
REL="HOME"
TITLE="Herramientas en GNU/Linux para estudiantes universitarios"
HREF="index.html"><LINK
REL="UP"
TITLE="Tipos de Redes Neuronales"
HREF="c81.html"><LINK
REL="PREVIOUS"
TITLE="Clasificación de las RNA"
HREF="x84.html"><LINK
REL="NEXT"
TITLE="Redes Autoorganizadas. Redes SOFM"
HREF="x152.html"><LINK
REL="stylesheet"
HREF="./base.css"
TYPE="text/css"></HEAD
><BODY
CLASS="SECT1"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>Herramientas en GNU/Linux para estudiantes universitarios: </TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="x84.html"
ACCESSKEY="P"
>Anterior</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
>Capítulo 3. Tipos de Redes Neuronales</TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
><A
HREF="x152.html"
ACCESSKEY="N"
>Siguiente</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="AEN105"
></A
>3.2. El perceptron multicapa (MLP)</H1
><P
>Este es uno de los tipos de redes más comunes. Se basa en otra red mas simple llamada
perceptrón simple solo que el número de capas ocultas puede ser mayor o igual que una.
Es una red unidireccional (feedforward). La arquitectura típica de esta red es la
siguiente:
       <DIV
CLASS="FIGURE"
><A
NAME="AEN108"
></A
><P
><B
>Figura 3-2. Representación de un Perceptrón Multicapa (MLP)</B
></P
><P
><IMG
SRC="mlp2.jpg"
ALIGN="CENTER"></P
></DIV
>
       </P
><P
>Las neuronas de la capa oculta usan como regla de propagación la suma ponderada de las
entradas con los pesos sinápticos w<SUB
>ij</SUB
> y sobre esa suma ponderada
se aplica una función de transferencia de tipo sigmoide, que es acotada en respuesta.
       <DIV
CLASS="FIGURE"
><A
NAME="AEN113"
></A
><P
><B
>Figura 3-3. Forma funcional de una sigmoide</B
></P
><P
><IMG
SRC="sigmoide.jpg"
ALIGN="CENTER"></P
></DIV
>
       </P
><P
>El aprendizaje que se suele usar en este tipo de redes recibe el nombre de
retropropagacion del error (backpropagation). Como funcion de coste global, se usa el
error cuadratico medio. Es decir, que dado un par 
(<B
CLASS="KEYCAP"
>x</B
><SUB
>k</SUB
>,
<B
CLASS="KEYCAP"
>d</B
><SUB
>k</SUB
>) 
correspondiente a la entrada k de los datos de entrenamiento y salida deseada asociada
se calcula la cantidad:
 <DIV
CLASS="EQUATION"
><A
NAME="AEN121"
></A
><P
><B
>Ecuación 3-1. Error cuadrático medio</B
></P
><IMG
SRC="Errorcuadraticomedio.jpg"
ALIGN="CENTER"></DIV
>
que vemos que es la suma de los errores parciales debido a cada patrón (índice p), resultantes
de la diferencia entre la salida deseada d<SUB
>p</SUB
> y la salida que da la red
f(.) ante el vector de entrada x<SUB
>k</SUB
>.
Si estas salidas son muy diferentes de las salidas deseadas, el error cuadratico medio sera grande.
f es la función de activación de las neuronas de la capa de salida e y la salida que proporcionan
las neuronas de la ultima capa oculta. 
       </P
><P
>Sobre esta función de coste global se aplica algun procedimiento de minimización. En el
caso del MLP se hace mediante un descenso por gradiente. Las expresiones que resultan
aplicando la regla de la cadena son las siguientes:
 <DIV
CLASS="EQUATION"
><A
NAME="AEN128"
></A
><P
><B
>Ecuación 3-2. Términos delta</B
></P
><IMG
SRC="terminosdelta.jpg"
ALIGN="CENTER"></DIV
>
Siendo y<SUB
>k</SUB
> las salidas de la capa oculta.
       </P
><P
>El aprendizaje por backpropagation queda como sigue:
       <P
></P
><OL
TYPE="1"
><LI
><P
>Inicializar los pesos y los umbrales iniciales de cada neurona. Hay varias posibilidades
de inicialización siendo las mas comunes las que introducen valores aleatorios pequeños.
     </P
></LI
><LI
><P
>Para cada patrón del conjunto de  los datos de entrenamiento
       <P
></P
><OL
TYPE="a"
><LI
><P
>Obtener la respuesta de la red ante ese patrón. Esta parte se consigue propagando la
entrada hacia adelante, ya que este tipo de red es feedforward. Las salidas de una capa
sirven como entrada a las neuronas de la capa siguiente, procesandolas de acuerdo a la
regla de propagación y la función de activación correspondientes.
             </P
></LI
><LI
><P
>Calcular los errores asociados según la ecuación 3-2
             </P
></LI
><LI
><P
>Calcular los incrementos parciales (sumandos de los sumatorios). Estos incrementos
dependen de los errores calculados en 2.b
             </P
></LI
></OL
>
     </P
></LI
><LI
><P
>Calcular el incremento total ,para todos los patrones, de los pesos y los umbrales
según las expresiones en la ecuación 3-2
     </P
></LI
><LI
><P
>Actualizar pesos y umbrales
     </P
></LI
><LI
><P
>Calcular el error actual y volver al paso 2 si no es satisfactorio.
     </P
></LI
></OL
>
       </P
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="x84.html"
ACCESSKEY="P"
>Anterior</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
ACCESSKEY="H"
>Inicio</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="x152.html"
ACCESSKEY="N"
>Siguiente</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Clasificación de las RNA</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="c81.html"
ACCESSKEY="U"
>Subir</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>Redes Autoorganizadas. Redes SOFM</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>