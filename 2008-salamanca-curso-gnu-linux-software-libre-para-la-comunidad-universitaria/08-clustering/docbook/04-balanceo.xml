<?xml version='1.0' encoding='utf-8'?>
  <chapter>
    <title>Balanceo de Carga</title>
    <para>Se puede definir el balanceo de carga como la habilidad que tiene un servicio para repartir el trabajo a realizar entre varias m&aacute;quinas.</para>
    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/balanceador.png" format="PNG"/>
      </imageobject>
	<caption>
          <para>Arquitectura b&aacute;sica de un cluster con balanceo de carga.</para>
	</caption>
    </mediaobject>
    <para>Los balancearores pueden ser tanto hardware como software.</para>
    <sect1>
      <title>Balanceadores hardware</title>
      <para>Los balancedares hardware son m&aacute;quinas con un prop&oacute;sito espec&iacute;fico y solo son &uacute;tiles para el balanceo de carga.</para>
      <sect2>
        <title>Ventajas</title>
        <itemizedlist>
          <listitem>
            <para>Potencia.</para>
          </listitem>
          <listitem>
            <para>Estabilidad.</para>
          </listitem>
          <listitem>
            <para>Escalabilidad.</para>
          </listitem>
        </itemizedlist>
      </sect2>
      <sect2>
        <title>Inconvenientes</title>
        <itemizedlist>
          <listitem>
            <para>Precio (equipo, mantenimiento, t&eacute;cnicos).</para>
          </listitem>
          <listitem>
            <para>S&oacute;lo sirve para balanceo.</para>
          </listitem>
        </itemizedlist>
      </sect2>
    </sect1>
    <sect1>
      <title>Balanceadores software</title>
      <para>Los balanceadores software son servidores configurados para hacer balanceo.</para>
      <sect2>
        <title>Ventajas</title>
        <itemizedlist>
          <listitem>
            <para>Precio.</para>
          </listitem>
          <listitem>
            <para>Una vez no sea necesario el balanceador o se requiera uno m&aacute;s potente se puede reciclar para otras tareas.</para>
          </listitem>
        </itemizedlist>
      </sect2>
      <sect2>
        <title>Inconvenientes</title>
        <itemizedlist>
          <listitem>
            <para>Mayor tiempo de mantenimiento.</para>
          </listitem>
          <listitem>
            <para>Menor potencia.</para>
          </listitem>
        </itemizedlist>
      </sect2>
    </sect1>
    <sect1>
      <title>Balanceo en DNS</title>
      <para>A partir de la version 9 de BIND nos bastaria simplemte con a&ntilde;adir varias entradas "IN A", una para cada servidor, que apunten al mismo nombre, el del servidor que queremos balancear:</para>
<programlisting>
www	IN	A	192.168.0.100
www	IN	A	192.168.0.101
www	IN	A	192.168.0.102
</programlisting>

      <para>Utilizando el comando "host" varias veces podemos ver como se balancean las peticiones dns:</para>
<screen>
<prompt>[jadebustos@dedalo ~]$ </prompt><userinput>host www.servidorbalanceado.es</userinput>
<computeroutput>www.servidorbalanceado.es has address 192.168.0.100
www.servidorbalanceado.es has address 192.168.0.101
www.servidorbalanceado.es has address 192.168.0.102</computeroutput>
<prompt>[jadebustos@dedalo ~]$ </prompt><userinput>host www.servidorbalanceado.es</userinput>
<computeroutput>www.servidorbalanceado.es has address 192.168.0.101
www.servidorbalanceado.es has address 192.168.0.102
www.servidorbalanceado.es has address 192.168.0.100</computeroutput>
<prompt>[jadebustos@dedalo ~]$ </prompt><userinput>host www.servidorbalanceado.es</userinput>
<computeroutput>www.servidorbalanceado.es has address 192.168.0.100
www.servidorbalanceado.es has address 192.168.0.101
www.servidorbalanceado.es has address 192.168.0.102</computeroutput>
<prompt>[jadebustos@dedalo ~]$</prompt>
</screen>
      <para>De esta forma cada vez que se haga una solicitud DNS para www.servidorbalanceado.es se resolver&aacute; con una IP diferente, ir&aacute; rotando.</para>

<para>Este sistema tiene varios inconvenientes:</para>

<orderedlist>
  <listitem>
    <para> El primero es que si un servidor cae el DNS no ser&aacute; consciente de ello y el cliente que obtenga la IP del servidor caido como respuesta a su solicitud encontrar&aacute; que el servicio no esta disponible.</para>
  </listitem>
  <listitem>
    <para>El segundo inconveniente es que con las caches de DNS se cachear&aacute; permanente el nombre <emphasis>www.servidorbalanceado.es</emphasis> con una IP especifica, con lo cual todas las peticiones iran a esa IP y no habr&aacute; balanceo, pudiendo de esta forma estar uno o varios servidores sobrecargados y el resto muy ligeros.</para>
  </listitem>
</orderedlist>
    </sect1>
    <sect1>
      <title>Linux Virtual Server - LVS</title>
      <para><ulink url="http://www.linuxvirtualserver.org">Linux Virtual Server</ulink> es una soluci&oacute;n para poder implementar un servidor virtual altamente escalable y en alta disponibilidad.</para> 

      <para>Esta soluci&oacute;n consiste en un balanceador de carga, tambi&eacute;n conocido como director, que ser&aacute; la m&aacute;quina que ser&aacute; accesible directamente para los clientes y luego tendremos los servidores que ser&aacute;n aquellos que recibiran las peticiones de los clientes, v&iacute;a el balanceador de carga, y responder&aacute;n a las peticiones.</para>

      <important><para>Los servidores podr&aacute;n estar o bien en la misma red f&iacute;sica o en redes diferentes lo que permitir&aacute; el tener servidores en granjas distribuidas geogr&aacute;ficamente.</para>

      <para>Esta soluci&oacute;n nos permitir&aacute; tener el servicio funcionando casi continuamente ya que no se ver&aacute; afectado por posibles ca&iacute;das de las m&aacute;quinas debido a fallos en el suministro el&eacute;ctrico o bien cortes en el ISP de una determinada granja. Cualquiera de estos fallos, u otros que pudieran ocurrir, afectar&aacute;n a una o varias granjas, pero nunca a todas con lo cual el servicio seguir&aacute;a funcionando aunque los clientes podr&aacute;an experimentar cierta demora en el servicio.</para></important>

      <para>Para los clientes existir&aacute; un &uacute;nico servidor (el balanceador) que se encargar&aacute; de distribuir la carga entre los servidores reales.</para>

      <para>La escalabilidad en el servicio la conseguiremos a&ntilde;adiendo nodos, mientras que la disponibilidad se lograr&aacute; identificando el nodo o el balanceador que no funciona y reconfigurando el sistema de tal forma que el servicio no se vea interrumpido. Es decir no enviando peticiones a un nodo que no pudiera dar servicio en ese momento.</para>

      <para>El balanceo lo podemos hacer de tres formas:</para>
      <itemizedlist>
        <listitem>
          <para>Mediante <emphasis>NAT</emphasis></para>
	</listitem>
        <listitem>
          <para><emphasis>IP Tunneling</emphasis></para>
	</listitem>
        <listitem>
          <para><emphasis>Direct Routing</emphasis></para>
	</listitem>
      </itemizedlist>
      <sect2>
        <title>Virtual Server mediante <emphasis>NAT</emphasis></title>
        <para><emphasis>NAT</emphasis> (<emphasis>N</emphasis>etwork <emphasis>A</emphasis>ddress <emphasis>T</emphasis>ranslation) es una t&eacute;cnica utilizada para que una m&aacute;quina reciba informaci&oacute;n dirigida a otra y esta pueda reenviarla a quien la solicit&oacute; inicialmente.</para> 

        <para>Para ello la m&aacute;quina que recibe la informaci&oacute;n, en forma de paquetes, deber&aacute; reescribir los paquetes sustituyendo su propia direcci&oacute;n con la de la m&aacute;quina que realiz&oacute; la petici&oacute;n (nos referimos a direcciones tanto f&iacute;sicas, MAC, como l&oacute;gicas, IP) una vez reescrito el paquete de la forma correcta el balanceador se encargar&aacute; de enviar los paquetes por la interface adecuada para que le lleguen al destino verdadero del paquete.</para>

        <para>Cuando el balanceador reciba peticiones este sobreescribir&aacute; el paquete y pondr&aacute; la direcci&oacute;n de un servidor real, gracias a esto los servidores reales podr&aacute;n estar ejecutando cualquier sistema operativo que sea accesible v&iacute;a TCP/IP.</para> 
 
        <para>Cuando el servidor responda lo har&aacute; al balanceador y este reescribir&aacute; el paquete, otra vez, poniendo en los paquetes la direcci&oacute;n del cliente que solicit&oacute; la informaci&oacute;n.</para> 
    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/lvs-nat.png" format="PNG"/>
      </imageobject>
	<caption>
          <para>LVS - NAT.</para>
	</caption>
    </mediaobject>

        <para>El balanceador guardar&aacute; los datos de todas las conexiones que balancee para luego devolver la respuesta al cliente adecuado.</para>

        <para>Pero no todo son ventajas, esta sobreescritura de los paquetes trae consigo una carga de CPU que puede llegar a ser un cuello de botella. Adem&aacute;s tendremos que tener en cuenta cual es el ancho real de nuestra interface de red y tener presente que por el balanceador van a pasar tanto las peticiones hacia los servidores, como las respuestas de los servidores hacia los clientes.</para> 

        <para>Todos estos paquetes tendr&aacute;n que ser reescritos por el balanceador y aunque aumentemos la memoria o las capacidades de procesamiento del balanceador todav&iacute;a estaremos limitados por el ancho real de la interface ya que las respuestas de los servidores ocupar&aacute;n mas ancho de banda que las peticiones.</para>

        <tip><para>El problema del ancho de banda se podr&aacute; paliar utilizando las capacidades de Bonding del n&uacute;cleo de Linux.</para></tip>

        <para>No suele ser muy recomendable esta opci&oacute;n ya que los costes necesarios para desplegar la infraestructura suelen ser mayores que los de implementar LVS con IP Tunneling o Direct Routing.</para> 

        <para>El balanceador deber&aacute; tener dos IP una de cara a los posibles clientes (DIP) y otra en la red de los servidores (VIP), es decir que el balanceador deber&aacute; hacer funciones de enrutado con lo cual el n&uacute;cleo deber&aacute; estar configurado para ello y tendremos que tener el enrutado habilitado y el n&uacute;cleo tendr&aacute; que estar compilado para poder sobreescribir paquetes.</para>

        <important><para>Para tener habilitado el enrutado es necesario que el n&uacute;cleo est&eacute; configurado para ello y necesitaremos que el fichero <filename>/proc/sys/net/ipv4/ip_forward</filename> si utilizamos <emphasis>ipv4</emphasis> o <filename>/proc/sys/net/ipv6/conf/all/forwarding</filename> est&eacute;n a <emphasis>1</emphasis>.</para></important>

        <important><para>Los servidores en este caso estar&aacute;n en la misma red de <emphasis>VIP</emphasis> y tendr&aacute;n como gateway al balanceador de carga.</para></important>
  
      </sect2>
      <sect2>
        <title>Virtual Server mediante <emphasis>IP Tunneling</emphasis></title>
        <para>Utilizando <emphasis>NAT</emphasis> ten&iacute;amos un cuello de botella en el balanceador ya que tiene que reescribir y distribuir los paquetes del cliente al servidor y viceversa.</para> 

        <para>Utilizando <emphasis>IP Tunneling</emphasis> el balanceador &uacute;nicamente tendr&aacute; que hacerse cargo de las peticiones de los clientes y enviarlas a los servidores, siendo estos mismos los que responder&aacute;n a los clientes. De esta forma el balanceador de carga puede manejar mas nodos, es decir el servicio es mas escalable.</para>

    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/LVS-IPTunneling.png" format="PNG" scale="50"/>
      </imageobject>
	<caption>
          <para>LVS - IP Tunneling.</para>
	</caption>
    </mediaobject>

        <para>Una vez el balanceador de carga tiene el paquete determina si pertenece a uno de los servicios que tiene balanceados. De ser as&iacute; encapsula el paquete en otro paquete y se lo env&iacute;a al servidor de destino. Es este el que se encarga de responder al cliente directametne sin pasar por el balanceador.</para>

        <para>El balanceador guarda una tabla de conexiones y cuando le llega un paquete determina si ya existe una conexi&oacute;n abierta y de ser as&iacute; que servidor real es el que est&aacute; sirviendola para enviarle el paquete.</para>

    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/VS-TUN-flow.png" format="PNG" scale="80"/>
      </imageobject>
	<caption>
          <para>Encapsulamiento IP en IP-Tunneling.</para>
	</caption>
    </mediaobject>

        <para>Los servidores deber&aacute;n estar configurados para trabajar con <emphasis>IP Tunneling (encapsulation)</emphasis> ya que cuando el balanceador recibe un paquete para uno de los servidores este lo encapsula en un datagrama IP y lo manda a uno de los servidores. Cuando el servidor lo recibe tend&aacute; que desencapsularlo y responder&aacute; directamente al cliente sin pasar por el balanceador con lo cual los servidores tendr&aacute;n que estar conectados tanto al balanceador como a los clientes (en NAT s&oacute;lo con el balanceador).</para>

        <tip><para>No es necesario que los servidores est&eacute;n en la misma red, pueden estar geogr&aacute;ficamente distribuidos.</para></tip>

        <important><para>En esta configuraci&oacute;n surge el problema de ARP.</para></important>

      </sect2>
      <sect2>
        <title>Virtual Server mediante <emphasis>Direct Routing</emphasis></title>

        <para>Al igual que en <emphasis>IP Tunneling</emphasis> el balanceador s&oacute;lo gestionar&aacute; las peticiones del cliente hac&iacute;a el servidor con lo cual es una soluci&oacute;n altamente escalable.</para>

        <para>La direcci&oacute;n virtual (VIP) es compartida por el balanceador y los servidores. De esta manera el balanceador recibe las peticiones y las env&iacute;a a los servidores que procesan las peticiones y dan servicio directamente a los clientes.</para> 

    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/lvs-dr.png" format="PNG" scale="50"/>
      </imageobject>
	<caption>
          <para>LVS - Direct Routing.</para>
	</caption>
    </mediaobject>

        <para>En esta soluci&oacute;n es necesario que una de las interfaces del balanceador y los servidores est&aacute;n en el mismo segmento f&iacute;sico de red ya que el balanceador de carga cambiar&aacute; su direcci&oacute;n f&iacute;sica, MAC, en la trama por la direcci&oacute;n f&iacute;sica de uno de los servidores que tendr&aacute; un alias con la direcci&oacute;n VIP.</para>

        <para>El balanceador guarda una tabla de conexiones y cuando le llega un paquete determina si ya existe una conexi&oacute;n abierta y de ser as&iacute; que servidor real es el que est&aacute; sirviendola para enviarle el paquete.</para>

    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/VS-DR-flow.png" format="PNG" scale="75"/>
      </imageobject>
	<caption>
          <para>LVS - Direct Routing.</para>
	</caption>
    </mediaobject>

    <important><para>El alias se acostumbra a poner en el dispositivo de loopback.</para></important>

    <important><para>En esta configuraci&oacute;n surge el problema de ARP.</para></important>

      </sect2>
      <sect2>
        <title>El problema del <emphasis>ARP</emphasis></title>
        <para>Cuando utilizamos <emphasis>Tunneling</emphasis> o <emphasis>Direct Routing</emphasis> tanto el balanceador como los servidores comparten una direcci&oacute;n IP (VIP) y esto puede traer consigo problemas si los balanceadores y los servidores est&aacute;n en la misma red.</para>

        <para>El modelo <emphasis>OSI</emphasis> de la <emphasis>ISO</emphasis> consta de 7 capas, las tres primeras son la capa f&iacute;sica, la capa de enlace de datos y la capa de red.</para>

        <para>Cuando un ordenador transmite datos empieza a encapsular en la capa de aplicaci&oacute;n, s&eacute;ptima capa. Cuando llega a la capa de red a&ntilde;ade la informaci&oacute;n de red, direcciones IP y direcciones l&oacute;gicas (origen y  destino). Acto seguido a&ntilde;ade su direcci&oacute;n f&iacute;sica o MAC que es una direcci&oacute;n &uacute;nica que tiene cada tarjeta de red o NIC y que consta de dos partes una que identifica al fabricante de la tarjeta y la otra parte identifica a la tarjeta.</para> 

        <para>Despu&eacute;s en la capa f&iacute;sica se traduce toda la informaci&oacute;n a se&ntilde;ales el&eacute;ctricas y se transmite por el medio. Adem&aacute;s de sus direcciones propias IP y MAC se a&ntilde;aden las direcciones del destinatario y si el paquete fuera destinado a una red diferente de la de partida se pondr&iacute;a en el campo de la MAC de destino la MAC de gateway o del router por defecto y este se ir&iacute;a encargando de enrutar el paquete hasta que llegar&aacute; al &uacute;ltimo router o gateway el cual cambiar&iacute;a su MAC por la MAC del equipo que tuviera la IP de destino del paquete. Este &uacute;ltimo router mandar&iacute;a el paquete por la interface correspondiente y todos los equipos en esa red desencapsular&iacute;n el paquete hasta la segunda capa y s&oacute;lo aquel cuya MAC este en esa trama, como destino, tomar&aacute; el paquete y lo desencapsulara entero para hacer uso de el. </para>

        <para>Cuando utilizamos <emphasis>Tunneling</emphasis> o <emphasis>Direct Routing</emphasis> tenemos que tener en cuenta que los clientes hacen las peticiones al balanceador, pero sin embargo las respuestas las reciben de los servidores.</para> 

        <important><para>Tanto el balanceador de carga como los servidores comparten una IP (VIP), cuando un cliente solicita una conexi&oacute;n con VIP la petici&oacute;n se debe de hacer al balanceador, no a los clientes.</para></important> 

        <para>Cuando llega una petici&oacute;n de un cliente para el servicio bajo <emphasis>LVS</emphasis> esta llegar&aacute; desde fuera de la red, con lo cual el router de esa red har&aacute; una petici&oacute;n ARP para obtener la MAC de la m&aacute;quina con la IP VIP.</para> 

        <para>En esa red podr&iacute;a haber varias m&aacute;quinas con la IP VIP (el balanceador y los servidores comparten dicha IP) con lo cual cualquiera de ellas podr&iacute;a, y de hecho lo har&aacute;, responder a la petici&oacute;n. Pero el paquete deber&aacute; ir destinado al balanceador no a los servidores.</para> 

        <para>El balanceador registrar&aacute; en sus tablas a que servidor le manda las peticiones y consecuentemente todas las peticiones de ese cliente ir&aacute;n al mismo servidor, bajo la conexi&oacute;n ya establecida. Si uno de los servidores respondiera a la petici&oacute;n ARP el router tendr&iacute;a en su tabla ARP la direcci&oacute;n f&iacute;sica del servidor y todos los paquetes se los enviar&aacute;  directamente al servidor sin utilizar el balanceador.</para> 

        <para>Si en alg&uacute;n momento se cambiar&aacute; la entrada en la tabla ARP y el router actualizar&aacute; con la MAC de otra m&aacute;quina (el balanceador y el resto de servidores tienen una interface o alias con la IP VIP) entonces las peticiones de ese cliente iran a otro servidor en lugar de al servidor que originariamente estaban yendo. Si esto pasa dentro de una misma conexi&oacute;n cuando un servidor empiece a recibir las solicitudes de una conexi&oacute;n que el no ha iniciado (la realiz&oacute; el servidor que primero respondi&oacute; a la petici&oacute;n ARP) la conexi&oacute;n se cerrar&aacute; y habr&aacute; que volver a negociarla.</para>

      <para>Este problema se presenta con n&uacute;cleos a partir de la serie 2.2.x y se soluciona haciendo que la interface que tiene la IP VIP no responda a peticiones ARP en los servidores y si en el balanceador de carga. De esta forma nos aseguramos que cuando el router haga una petici&oacute;n ARP para la VIP la &uacute;nica m&aacute;quina que responda sea el balanceador y de esta forma todos los paquetes para el LVS ser&aacute;n enviados al balanceador y este har&aacute; su trabajo.</para>

      </sect2>
      <sect2>
        <title>Algoritmos de planificaci&oacute;n en <emphasis>LVS</emphasis></title>

        <para>Hemos estado haciendo referencia a que el balanceador distribuir&aacute; las peticiones entre los servidores. Pero para que esta distribuci&oacute;n sea efectiva ha de ser planificada de alguna forma. A la hora de compilar el n&uacute;cleo en el balanceador tendremos que escoger que algoritmos vamos a utilizar para hacer el balanceo de carga.</para>

        <para>Los algoritmos m&aacute;s interesantes son los siguientes:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis>Round-Robin</emphasis>. Este algoritmo es el m&aacute;s simple y lo que hace es distribuir las peticiones entre los servidores de tal manera que si hay 5 servidores y 100 peticiones cada servidor atender&aacute; a 20 peticiones.</para> 

            <para>El orden de distribuci&oacute;n de la carga ser&aacute; secuencial, primera petici&oacute;n hac&iacute;a el primer servidor, segunda al segundo, ...,  quinta al quinto, sexta al primero, ...</para>

            <para>Esta distribuci&oacute;n es muy sencilla pero presupone que todas las peticiones van a ser equivalentes, en t&eacute;rminos de carga, para el servidor, algo que en la realidad dista mucho de ser cierto. O que la capacidad de procesamiento de los servidores es la misma.</para> 

            <para>Podr&iacute;a darse el caso de que haya servidores atendiendo varias peticiones y otros esperando o que los servidores m&aacute;s lentos estuvieran muy sobrecargados mientras que los m&aacute;s potentes estuvieran m&aacute;s desahogados.</para>
          </listitem>
          <listitem>
            <para><emphasis>Weighted Round-Robin</emphasis>. Este algoritmo permite un aprovechamiento mejor del cluster cuando hay m&aacute;quinas con diferentes capacidades de procesamiento, de esta forma a las m&aacute;quinas con mayor capacidad de procesamiento se les dar&aacute; una mayor prioridad (weight) para responder a las peticiones de los clientes y el balanceador distribuir&aacute; la carga entre los servidores teniendo en cuenta su prioridad.</para>

            <para>En realidad el Round-Robin Scheduling es un Weighted Round-Robin Scheduling y todas las prioridades son iguales para los servidores.</para>
          </listitem>
          <listitem>
            <para><emphasis>Least-Connection</emphasis>. Con este algoritmo las peticiones se enviaran al servidor que menos conexiones este sirviendo en ese momento.</para> 

            <para>Si la capacidad de procesamiento de los servidores es similar este algoritmo distribuir&aacute; la carga de forma &oacute;ptima entre todas las m&aacute;quinas del cluster. Sin embargo si las capacidades de procesamiento var&iacute;an mucho la carga no sera repartida de forma ecu&aacute;nime ya que la carga se repartir&aacute; seg&uacute;n el n&uacute;mero de conexiones abiertas en ese momento y no sobre la carga real de cada m&aacute;quina.</para>
          </listitem>
          <listitem>
            <para><emphasis>Weighted Least-Connection</emphasis>. Este algoritmo es al <emphasis>Least-Connection Scheduling</emphasis> lo que el <emphasis>Weighted Round-Robin Scheduling</emphasis> es al <emphasis>Round-Robin Scheduling</emphasis>.</para> 

            <para>A cada servidor se le asigna una prioridad seg&uacute;n su capacidad de procesamiento y aquellos que mas prioridad tengan asignada atender&aacute;n m&aacute;s peticiones, es decir tendr&uacute;n m&uacute;s conexiones abiertas.</para>
          </listitem>
        </itemizedlist>
      </sect2>
      <sect2>
        <title>ipvsadm</title>
        <para><command>ipvsadm</command> es una herramienta en espacio de usuario para interactuar con <emphasis>LVS</emphasis>.</para>

        <para>Podemos ver el listado de conexiones:</para>
<screen>
<prompt>[jadebustos@dedalo ~]# </prompt><userinput>ipvsadm -Ln</userinput>
<computeroutput>IP Virtual Server version 1.2.0 (size=4096) 
Prot LocalAddress:Port Scheduler Flags 
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn 
TCP  172.16.0.207:443 wlc persistent 3600 
  -> 172.16.0.199:443             Route   1      5          0 
  -> 172.16.0.198:443             Route   1      2          0 
TCP  172.16.0.205:9085 wlc 
  -> 172.16.0.200:9085            Route   1      2          0 
  -> 172.16.0.201:9085            Route   1      2          0 
TCP  172.16.0.206:80 wlc 
  -> 172.16.0.197:80              Route   1      1          0 
  -> 172.16.0.195:80              Route   1      4          0 
  -> 172.16.0.196:80              Route   1      2          0 
TCP  172.16.0.207:80 wlc persistent 3600 
  -> 172.16.0.199:80              Route   1      1          0 
  -> 172.16.0.198:80              Route   1      1          0 
TCP  172.16.0.205:80 wlc 
  -> 172.16.0.200:80              Route   1      1          0 
  -> 172.16.0.201:80              Route   1      1          0</computeroutput>
<prompt>[jadebustos@dedalo ~]#</prompt>
</screen> 

        <para>Podemos sacar estad&iacute;sticas:</para>
<screen>
<prompt>[jadebustos@dedalo ~]# </prompt><userinput>ipvsadm --list --stats --numeric</userinput>
<computeroutput>IP Virtual Server version 1.2.0 (size=4096) 
Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes 
  -> RemoteAddress:Port 
TCP  172.16.0.207:443                    7        169     292       x        y 
  -> 172.16.0.199:443                    5        113     214       x        y 
  -> 172.16.0.198:443                    2         56      78       x        y 
TCP  172.16.0.205:9085                   4         67      99       x        y 
  -> 172.16.0.200:9085                   2         34      45       x        y 
  -> 172.16.0.201:9085                   2         33      44       x        y 
TCP  172.16.0.206:80                     7        109     178       x        y 
  -> 172.16.0.197:80                     1         12      23       x        y 
  -> 172.16.0.195:80                     4         67     100       x        y 
  -> 172.16.0.196:80                     2         30      45       x        y 
TCP  172.16.0.207:80                     2         30      58       x        y 
  -> 172.16.0.199:80                     1         13      25       x        y 
  -> 172.16.0.198:80                     1         17      33       x        y 
TCP  172.16.0.205:80                     2         27      50       x        y  
  -> 172.16.0.200:80                     1         15      27       x        y 
  -> 172.16.0.201:80                     1         12      23       x        y </computeroutput>
<prompt>[jadebustos@dedalo ~]#</prompt>
</screen>        

      <para>Otras opciones que nos permite:</para>
      <itemizedlist>
        <listitem>
          <para>A&ntilde;adir, modificar y borrar servicios.</para>
        </listitem>
        <listitem>
          <para>A&ntilde;adir, modificar y borrar servidores.</para>
        </listitem>
        <listitem>
          <para>Modificar los par&aacute;metros de configuraci&oacute;n de los servicios balanceados.</para>
        </listitem>
      </itemizedlist>
      </sect2>
      <sect2>
        <title>Alta disponibilidad en los balanceadores con <emphasis>keepalived</emphasis></title>
        <para>El balanceador es un punto de fallo cr&iacute;tico. Si el balanceador no est&aacute; disponible no habr&aacute; servicio aunque las m&aacute;quinas que den el servicio est&eacute;n en perfecto estado de funcionamiento.</para>

    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/balanceador-2.png" format="PNG"/>
      </imageobject>
	<caption>
          <para>Fallo en balanceador.</para>
	</caption>
    </mediaobject>

        <para>La alta disponibilidad se logr&aacute; garantizando que siempre haya un balanceador funcionando, resumiendo el servicio siempre tiene que estar disponible.</para>

    <mediaobject>
      <imageobject>
	<imagedata align="center" fileref="img/04-balanceo/balanceador-ha.png" format="PNG" scale="50"/>
      </imageobject>
	<caption>
          <para>Balanceadores en Alta Disponibilidad.</para>
	</caption>
    </mediaobject>

        <para><ulink url="http://www.keepalived.org">Keepalived</ulink> es un demonio que se encarga de que siempre haya un balanceador balanceanado el servicio (<emphasis>failover</emphasis>). En realidad es un interface a <emphasis>LVS</emphasis>.</para>

        <para>Adem&aacute;s tambi&eacute;n se encarga de que no se env&iacute;en peticiones a servidores que en ese momento no puedan atenderlas (<emphasis>health-checking</emphasis>).</para>

        <para><emphasis>Keepalived</emphasis> se instala en los balanceadores. Uno de ellos ser&aacute; el <emphasis>MASTER</emphasis> y el resto ser&aacute;n denominados de <emphasis>BACKUP</emphasis>. Cuando el <emphasis>MASTER</emphasis> deje de estar operativo entrar&aacute; en funcionamiento el balanceador <emphasis>BACKUP</emphasis> de m&aacute;s alta prioridad y continuar&aacute; balanceando hasta que el <emphasis>MASTER</emphasis> vuelva a estar operativo o hasta que tenga alg&uacute;n problema, entonces lo sustituir&aacute; otro balanceador de <emphasis>BACKUP</emphasis>, el siguiente de m&aacute;s alta prioridad. De esta forma siempre estar&aacute; balanceando el balanceador de m&aacute;s alta prioridad. En el momento que un balanceador de m&aacute;s alta prioridad que el activo vuelva al servicio asumir&aacute; el balanceo.</para>

      <para><emphasis>Keepalived</emphasis> utiliza el protocolo <emphasis>VRRP</emphasis> (rfc 2338). <emphasis>Keepalived</emphasis> formar&aacute; un balanceador virtual formado por un balanceador <emphasis>MASTER</emphasis> y varios balanceadores <emphasis>BACKUP</emphasis> funcionando de la manera ant&eacute;s descrita.</para> 

      <important><para>Los balanceadores informar&aacute;n al resto de su disponibilidad utilizando el protocolo <emphasis>VRRP</emphasis>.</para></important>

      <para>Para el <emphasis>health-checking</emphasis> de los servicios balanceados <emphasis>keepalived</emphasis> puede realizar la comprobaci&oacute;n de varias formas:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis>TCP_CHECK</emphasis> se har&aacute; una petici&oacute;n TCP y si no es respondida se eliminar&aacute; el servidor de la lista de servidores activos.</para>
        </listitem>
        <listitem>
          <para><emphasis>HTTP_GET</emphasis> Se solicitar&aacute; una p&aacute;gina del servidor y se comprobar&aacute; con una que es la correcta mediante un hashing MD5, previamente calculado,  en caso de no coincidir el servidor ser&aacute; eliminado de la lista de servidores activos.</para> 

          <important><para>Para general el hashing MD5 utilizaremos la utilidad <command>genhash</command> que nos permitir&aacute; generar el hashing directamente desde el servidor.</para></important>
        </listitem>
        <listitem>
          <para><emphasis>SSL_GET</emphasis> igual que <emphasis>HTTP_GET</emphasis> pero la p&aacute;gina ser&aacute; solicitada bajo una conexi&oacute;n SSL por el puerto 443 (https).</para>
        </listitem>
        <listitem>
          <para><emphasis>MISC_CHECK</emphasis> esta opci&oacute;n nos permitir&aacute; comprobar la disponibilidad de los servidores mediante un script creado por nosotros. Si la situaci&oacute;n lo  requiere podremos comprobar la disponibilidad de los servidores de forma personalizada.</para>
        </listitem>
      </itemizedlist>
      </sect2>
      <sect2>
        <title>Configuraci&oacute;n de <emphasis>keepalived</emphasis></title>
        <para>El fichero de configuraci&oacute;n de <emphasis>keepalived</emphasis> normalmente en <filename>/etc/keepalived.conf</filename> consta de tres partes:</para>
        <orderedlist>
          <listitem>
            <para><emphasis>Zona de configuraci&oacute;n global</emphasis> donde se especificar&aacute; la forma en la que <emphasis>keepalived</emphasis> avisar&aacute; a los administradores de un fallo en el servidor virtual (direcci&oacute;n de correo, servidor SMTP, ...)</para>
	  </listitem>
          <listitem>
            <para><emphasis>Configuraci&oacute;n de VRRP</emphasis> donde indicaremos si el balanceador es <emphasis>MASTER</emphasis> o <emphasis>BACKUP</emphasis>, la prioridad, cual ser&aacute; la interface por la que tiene acceso al servidor virtual, la IP virtual (VIP) ...</para>
	  </listitem>
          <listitem>
            <para><emphasis>Configuraci&oacute;n del servidor virtual</emphasis> indicando la IP (VIP) y el puerto, protocolo, ... y una entrada real server con los par&aacute;metros de cada servidor real, IP (real), puerto, m&eacute;todo de health-checking, ...</para>
	  </listitem>
        </orderedlist>
        <important><para>Este fichero ser&aacute; exactamente igual en el <emphasis>MASTER</emphasis> y en los <emphasis>BACKUPS</emphasis> salvo que en los <emphasis>BACKUPS</emphasis> el par&aacute;metro <emphasis>state</emphasis> en la configuraci&oacute;n de <emphasis>VRRP</emphasis> ser&aacute; <emphasis>BACKUP</emphasis> y en el <emphasis>MASTER</emphasis> ser&aacute; <emphasis>MASTER</emphasis> como cabr&iacute;a esperar.</para></important>

        <para>Un ejemplo t&iacute;pico del fichero <filename>/etc/keepalived.conf</filename> para una configuraci&oacute;n <emphasis>Direct Routing</emphasis>:</para>
<programlisting>
# Configuration File for keepalived

# Configuracion Global

global_defs {
   notification_email {
     alertas@midominio.com
   }
   notification_email_from balanceador1@midominio.com
   smtp_server 192.168.1.10
   smtp_connect_timeout 30
   lvs_id LVS_DEVEL
}

vrrp_instance wasIntranet {
    state MASTER
    interface eth1
    virtual_router_id 50
    priority 100
    advert_int 1
    wdog-vrrp 1
    virtual_ipaddress {
        172.16.0.205
	172.16.0.206
	172.16.0.207
    }
}

virtual_server 172.16.0.205 80{ 
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.200 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.201 80 {
        weight 1
        TCP_CHECK {
            connect_port 80 
            connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.205 9085 {
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.200 9085 {
        weight 1
        TCP_CHECK {
           connect_port 9085
           connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.201 9085 {
        weight 1
        TCP_CHECK {
           connect_port 9085
           connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.206 80 { 
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.195 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.196 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.197 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.207 80 { 
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.198 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.199 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.207 443 {
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.198 443 {
         weight 1
         TCP_CHECK {
            connect_port 443
            connect_timeout 30
         }
         delay_before_retry 3
    }
    real_server 172.16.0.199 443 {
         weight 1
         TCP_CHECK {
             connect_port 443
             connect_timeout 30
         }
         delay_before_retry 3
    }
}
</programlisting>
      </sect2>
    </sect1>
  </chapter>

<!-- Local Variables: -->
<!-- xml-parent-document: "clustering.xml" -->
<!-- End: -->

