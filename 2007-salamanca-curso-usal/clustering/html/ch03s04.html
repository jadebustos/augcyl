<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Linux Virtual Server - LVS</title><meta name="generator" content="DocBook XSL Stylesheets V1.71.0"><link rel="start" href="index.html" title="Utilización y Administración avanzadas de sistemas GNU/Linux y aplicaciones Software Libre para estudiantes universitarios"><link rel="up" href="ch03.html" title="Capítulo 3. Balanceo de Carga"><link rel="prev" href="ch03s03.html" title="Balanceo en DNS"><link rel="next" href="ch04.html" title="Capítulo 4. Detección de fallos en los nodos del cluster."></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Linux Virtual Server - LVS</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ch03s03.html">Anterior</a> </td><th width="60%" align="center">Capítulo 3. Balanceo de Carga</th><td width="20%" align="right"> <a accesskey="n" href="ch04.html">Siguiente</a></td></tr></table><hr></div><div class="sect1" lang="es"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2545907"></a>Linux Virtual Server - LVS</h2></div></div></div><p><a href="http://www.linuxvirtualserver.org" target="_top">Linux Virtual Server</a> es una solución para poder implementar un servidor virtual altamente escalable y en alta disponibilidad.</p><p>Esta solución consiste en un balanceador de carga, también conocido como director, que será la máquina que será accesible directamente para los clientes y luego tendremos los servidores que serán aquellos que recibiran las peticiones de los clientes, vía el balanceador de carga, y responderán a las peticiones.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>Los servidores podrán estar o bien en la misma red física o en redes diferentes lo que permitirá el tener servidores en granjas distribuidas geográficamente.</p><p>Esta solución nos permitirá tener el servicio funcionando casi continuamente ya que no se verá afectado por posibles caídas de las máquinas debido a fallos en el suministro eléctrico o bien cortes en el ISP de una determinada granja. Cualquiera de estos fallos, u otros que pudieran ocurrir, afectarán a una o varias granjas, pero nunca a todas con lo cual el servicio seguiráa funcionando aunque los clientes podráan experimentar cierta demora en el servicio.</p></div><p>Para los clientes existirá un único servidor (el balanceador) que se encargará de distribuir la carga entre los servidores reales.</p><p>La escalabilidad en el servicio la conseguiremos añadiendo nodos, mientras que la disponibilidad se logrará identificando el nodo o el balanceador que no funciona y reconfigurando el sistema de tal forma que el servicio no se vea interrumpido. Es decir no enviando peticiones a un nodo que no pudiera dar servicio en ese momento.</p><p>El balanceo lo podemos hacer de tres formas:</p><div class="itemizedlist"><ul type="disc"><li><p>Mediante <span class="emphasis"><em>NAT</em></span></p></li><li><p><span class="emphasis"><em>IP Tunneling</em></span></p></li><li><p><span class="emphasis"><em>Direct Routing</em></span></p></li></ul></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2545995"></a>Virtual Server mediante <span class="emphasis"><em>NAT</em></span></h3></div></div></div><p><span class="emphasis"><em>NAT</em></span> (<span class="emphasis"><em>N</em></span>etwork <span class="emphasis"><em>A</em></span>ddress <span class="emphasis"><em>T</em></span>ranslation) es una técnica utilizada para que una máquina reciba información dirigida a otra y esta pueda reenviarla a quien la solicitó inicialmente.</p><p>Para ello la máquina que recibe la información, en forma de paquetes, deberá reescribir los paquetes sustituyendo su propia dirección con la de la máquina que realizó la petición (nos referimos a direcciones tanto físicas, MAC, como lógicas, IP) una vez reescrito el paquete de la forma correcta el balanceador se encargará de enviar los paquetes por la interface adecuada para que le lleguen al destino verdadero del paquete.</p><p>Cuando el balanceador reciba peticiones este sobreescribirá el paquete y pondrá la dirección de un servidor real, gracias a esto los servidores reales podrán estar ejecutando cualquier sistema operativo que sea accesible vía TCP/IP.</p><p>Cuando el servidor responda lo hará al balanceador y este reescribirá el paquete, otra vez, poniendo en los paquetes la dirección del cliente que solicitó la información.</p><div class="mediaobject" align="center"><img src="images/lvs-nat.gif" align="middle"><div class="caption"><p>LVS - NAT.</p></div></div><p>El balanceador guardará los datos de todas las conexiones que balancee para luego devolver la respuesta al cliente adecuado.</p><p>Pero no todo son ventajas, esta sobreescritura de los paquetes trae consigo una carga de CPU que puede llegar a ser un cuello de botella. Además tendremos que tener en cuenta cual es el ancho real de nuestra interface de red y tener presente que por el balanceador van a pasar tanto las peticiones hacia los servidores, como las respuestas de los servidores hacia los clientes.</p><p>Todos estos paquetes tendrán que ser reescritos por el balanceador y aunque aumentemos la memoria o las capacidades de procesamiento del balanceador todavía estaremos limitados por el ancho real de la interface ya que las respuestas de los servidores ocuparán mas ancho de banda que las peticiones.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Sugerencia</h3><p>El problema del ancho de banda se podrá paliar utilizando las capacidades de Bonding del núcleo de Linux.</p></div><p>No suele ser muy recomendable esta opción ya que los costes necesarios para desplegar la infraestructura suelen ser mayores que los de implementar LVS con IP Tunneling o Direct Routing.</p><p>El balanceador deberá tener dos IP una de cara a los posibles clientes (DIP) y otra en la red de los servidores (VIP), es decir que el balanceador deberá hacer funciones de enrutado con lo cual el núcleo deberá estar configurado para ello y tendremos que tener el enrutado habilitado y el núcleo tendrá que estar compilado para poder sobreescribir paquetes.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>Para tener habilitado el enrutado es necesario que el núcleo esté configurado para ello y necesitaremos que el fichero <code class="filename">/proc/sys/net/ipv4/ip_forward</code> si utilizamos <span class="emphasis"><em>ipv4</em></span> o <code class="filename">/proc/sys/net/ipv6/conf/all/forwarding</code> estén a <span class="emphasis"><em>1</em></span>.</p></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>Los servidores en este caso estarán en la misma red de <span class="emphasis"><em>VIP</em></span> y tendrán como gateway al balanceador de carga.</p></div></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2546163"></a>Virtual Server mediante <span class="emphasis"><em>IP Tunneling</em></span></h3></div></div></div><p>Utilizando <span class="emphasis"><em>NAT</em></span> teníamos un cuello de botella en el balanceador ya que tiene que reescribir y distribuir los paquetes del cliente al servidor y viceversa.</p><p>Utilizando <span class="emphasis"><em>IP Tunneling</em></span> el balanceador únicamente tendrá que hacerse cargo de las peticiones de los clientes y enviarlas a los servidores, siendo estos mismos los que responderán a los clientes. De esta forma el balanceador de carga puede manejar mas nodos, es decir el servicio es mas escalable.</p><div class="mediaobject" align="center"><img src="images/LVS-IPTunneling.gif" align="middle"><div class="caption"><p>LVS - IP Tunneling.</p></div></div><p>Una vez el balanceador de carga tiene el paquete determina si pertenece a uno de los servicios que tiene balanceados. De ser así encapsula el paquete en otro paquete y se lo envía al servidor de destino. Es este el que se encarga de responder al cliente directametne sin pasar por el balanceador.</p><p>El balanceador guarda una tabla de conexiones y cuando le llega un paquete determina si ya existe una conexión abierta y de ser así que servidor real es el que está sirviendola para enviarle el paquete.</p><div class="mediaobject" align="center"><img src="images/VS-TUN-flow.jpg" align="middle"><div class="caption"><p>Encapsulamiento IP en IP-Tunneling.</p></div></div><p>Los servidores deberán estar configurados para trabajar con <span class="emphasis"><em>IP Tunneling (encapsulation)</em></span> ya que cuando el balanceador recibe un paquete para uno de los servidores este lo encapsula en un datagrama IP y lo manda a uno de los servidores. Cuando el servidor lo recibe tendá que desencapsularlo y responderá directamente al cliente sin pasar por el balanceador con lo cual los servidores tendrán que estar conectados tanto al balanceador como a los clientes (en NAT sólo con el balanceador).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Sugerencia</h3><p>No es necesario que los servidores estén en la misma red, pueden estar geográficamente distribuidos.</p></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>En esta configuración surge el problema de ARP.</p></div></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2546307"></a>Virtual Server mediante <span class="emphasis"><em>Direct Routing</em></span></h3></div></div></div><p>Al igual que en <span class="emphasis"><em>IP Tunneling</em></span> el balanceador sólo gestionará las peticiones del cliente hacía el servidor con lo cual es una solución altamente escalable.</p><p>La dirección virtual (VIP) es compartida por el balanceador y los servidores. De esta manera el balanceador recibe las peticiones y las envía a los servidores que procesan las peticiones y dan servicio directamente a los clientes.</p><div class="mediaobject" align="center"><img src="images/lvs-dr.gif" align="middle"><div class="caption"><p>LVS - Direct Routing.</p></div></div><p>En esta solución es necesario que una de las interfaces del balanceador y los servidores están en el mismo segmento físico de red ya que el balanceador de carga cambiará su dirección física, MAC, en la trama por la dirección física de uno de los servidores que tendrá un alias con la dirección VIP.</p><p>El balanceador guarda una tabla de conexiones y cuando le llega un paquete determina si ya existe una conexión abierta y de ser así que servidor real es el que está sirviendola para enviarle el paquete.</p><div class="mediaobject" align="center"><img src="images/VS-DR-flow.jpg" align="middle"><div class="caption"><p>LVS - Direct Routing.</p></div></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>El alias se acostumbra a poner en el dispositivo de loopback.</p></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>En esta configuración surge el problema de ARP.</p></div></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2546432"></a>El problema del <span class="emphasis"><em>ARP</em></span></h3></div></div></div><p>Cuando utilizamos <span class="emphasis"><em>Tunneling</em></span> o <span class="emphasis"><em>Direct Routing</em></span> tanto el balanceador como los servidores comparten una dirección IP (VIP) y esto puede traer consigo problemas si los balanceadores y los servidores están en la misma red.</p><p>El modelo <span class="emphasis"><em>OSI</em></span> de la <span class="emphasis"><em>ISO</em></span> consta de 7 capas, las tres primeras son la capa física, la capa de enlace de datos y la capa de red.</p><p>Cuando un ordenador transmite datos empieza a encapsular en la capa de aplicación, séptima capa. Cuando llega a la capa de red añade la información de red, direcciones IP y direcciones lógicas (origen y  destino). Acto seguido añade su dirección física o MAC que es una dirección única que tiene cada tarjeta de red o NIC y que consta de dos partes una que identifica al fabricante de la tarjeta y la otra parte identifica a la tarjeta.</p><p>Después en la capa física se traduce toda la información a señales eléctricas y se transmite por el medio. Además de sus direcciones propias IP y MAC se añaden las direcciones del destinatario y si el paquete fuera destinado a una red diferente de la de partida se pondría en el campo de la MAC de destino la MAC de gateway o del router por defecto y este se iría encargando de enrutar el paquete hasta que llegará al último router o gateway el cual cambiaría su MAC por la MAC del equipo que tuviera la IP de destino del paquete. Este último router mandaría el paquete por la interface correspondiente y todos los equipos en esa red desencapsularín el paquete hasta la segunda capa y sólo aquel cuya MAC este en esa trama, como destino, tomará el paquete y lo desencapsulara entero para hacer uso de el. </p><p>Cuando utilizamos <span class="emphasis"><em>Tunneling</em></span> o <span class="emphasis"><em>Direct Routing</em></span> tenemos que tener en cuenta que los clientes hacen las peticiones al balanceador, pero sin embargo las respuestas las reciben de los servidores.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>Tanto el balanceador de carga como los servidores comparten una IP (VIP), cuando un cliente solicita una conexión con VIP la petición se debe de hacer al balanceador, no a los clientes.</p></div><p>Cuando llega una petición de un cliente para el servicio bajo <span class="emphasis"><em>LVS</em></span> esta llegará desde fuera de la red, con lo cual el router de esa red hará una petición ARP para obtener la MAC de la máquina con la IP VIP.</p><p>En esa red podría haber varias máquinas con la IP VIP (el balanceador y los servidores comparten dicha IP) con lo cual cualquiera de ellas podría, y de hecho lo hará, responder a la petición. Pero el paquete deberá ir destinado al balanceador no a los servidores.</p><p>El balanceador registrará en sus tablas a que servidor le manda las peticiones y consecuentemente todas las peticiones de ese cliente irán al mismo servidor, bajo la conexión ya establecida. Si uno de los servidores respondiera a la petición ARP el router tendría en su tabla ARP la dirección física del servidor y todos los paquetes se los enviará  directamente al servidor sin utilizar el balanceador.</p><p>Si en algún momento se cambiará la entrada en la tabla ARP y el router actualizará con la MAC de otra máquina (el balanceador y el resto de servidores tienen una interface o alias con la IP VIP) entonces las peticiones de ese cliente iran a otro servidor en lugar de al servidor que originariamente estaban yendo. Si esto pasa dentro de una misma conexión cuando un servidor empiece a recibir las solicitudes de una conexión que el no ha iniciado (la realizó el servidor que primero respondió a la petición ARP) la conexión se cerrará y habrá que volver a negociarla.</p><p>Este problema se presenta con núcleos a partir de la serie 2.2.x y se soluciona haciendo que la interface que tiene la IP VIP no responda a peticiones ARP en los servidores y si en el balanceador de carga. De esta forma nos aseguramos que cuando el router haga una petición ARP para la VIP la única máquina que responda sea el balanceador y de esta forma todos los paquetes para el LVS serán enviados al balanceador y este hará su trabajo.</p></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2546569"></a>Algoritmos de planificación en <span class="emphasis"><em>LVS</em></span></h3></div></div></div><p>Hemos estado haciendo referencia a que el balanceador distribuirá las peticiones entre los servidores. Pero para que esta distribución sea efectiva ha de ser planificada de alguna forma. A la hora de compilar el núcleo en el balanceador tendremos que escoger que algoritmos vamos a utilizar para hacer el balanceo de carga.</p><p>Los algoritmos más interesantes son los siguientes:</p><div class="itemizedlist"><ul type="disc"><li><p><span class="emphasis"><em>Round-Robin</em></span>. Este algoritmo es el más simple y lo que hace es distribuir las peticiones entre los servidores de tal manera que si hay 5 servidores y 100 peticiones cada servidor atenderá a 20 peticiones.</p><p>El orden de distribución de la carga será secuencial, primera petición hacía el primer servidor, segunda al segundo, ...,  quinta al quinto, sexta al primero, ...</p><p>Esta distribución es muy sencilla pero presupone que todas las peticiones van a ser equivalentes, en términos de carga, para el servidor, algo que en la realidad dista mucho de ser cierto. O que la capacidad de procesamiento de los servidores es la misma.</p><p>Podría darse el caso de que haya servidores atendiendo varias peticiones y otros esperando o que los servidores más lentos estuvieran muy sobrecargados mientras que los más potentes estuvieran más desahogados.</p></li><li><p><span class="emphasis"><em>Weighted Round-Robin</em></span>. Este algoritmo permite un aprovechamiento mejor del cluster cuando hay máquinas con diferentes capacidades de procesamiento, de esta forma a las máquinas con mayor capacidad de procesamiento se les dará una mayor prioridad (weight) para responder a las peticiones de los clientes y el balanceador distribuirá la carga entre los servidores teniendo en cuenta su prioridad.</p><p>En realidad el Round-Robin Scheduling es un Weighted Round-Robin Scheduling y todas las prioridades son iguales para los servidores.</p></li><li><p><span class="emphasis"><em>Least-Connection</em></span>. Con este algoritmo las peticiones se enviaran al servidor que menos conexiones este sirviendo en ese momento.</p><p>Si la capacidad de procesamiento de los servidores es similar este algoritmo distribuirá la carga de forma óptima entre todas las máquinas del cluster. Sin embargo si las capacidades de procesamiento varían mucho la carga no sera repartida de forma ecuánime ya que la carga se repartirá según el número de conexiones abiertas en ese momento y no sobre la carga real de cada máquina.</p></li><li><p><span class="emphasis"><em>Weighted Least-Connection</em></span>. Este algoritmo es al <span class="emphasis"><em>Least-Connection Scheduling</em></span> lo que el <span class="emphasis"><em>Weighted Round-Robin Scheduling</em></span> es al <span class="emphasis"><em>Round-Robin Scheduling</em></span>.</p><p>A cada servidor se le asigna una prioridad según su capacidad de procesamiento y aquellos que mas prioridad tengan asignada atenderán más peticiones, es decir tendrún mús conexiones abiertas.</p></li></ul></div></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2546697"></a>ipvsadm</h3></div></div></div><p><span><strong class="command">ipvsadm</strong></span> es una herramienta en espacio de usuario para interactuar con <span class="emphasis"><em>LVS</em></span>.</p><p>Podemos ver el listado de conexiones:</p><pre class="screen">
<code class="prompt">[jadebustos@dedalo ~]# </code><strong class="userinput"><code>ipvsadm -Ln</code></strong>
<code class="computeroutput">IP Virtual Server version 1.2.0 (size=4096) 
Prot LocalAddress:Port Scheduler Flags 
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn 
TCP  172.16.0.207:443 wlc persistent 3600 
  -&gt; 172.16.0.199:443             Route   1      5          0 
  -&gt; 172.16.0.198:443             Route   1      2          0 
TCP  172.16.0.205:9085 wlc 
  -&gt; 172.16.0.200:9085            Route   1      2          0 
  -&gt; 172.16.0.201:9085            Route   1      2          0 
TCP  172.16.0.206:80 wlc 
  -&gt; 172.16.0.197:80              Route   1      1          0 
  -&gt; 172.16.0.195:80              Route   1      4          0 
  -&gt; 172.16.0.196:80              Route   1      2          0 
TCP  172.16.0.207:80 wlc persistent 3600 
  -&gt; 172.16.0.199:80              Route   1      1          0 
  -&gt; 172.16.0.198:80              Route   1      1          0 
TCP  172.16.0.205:80 wlc 
  -&gt; 172.16.0.200:80              Route   1      1          0 
  -&gt; 172.16.0.201:80              Route   1      1          0</code>
<code class="prompt">[jadebustos@dedalo ~]#</code>
</pre><p>Podemos sacar estadísticas:</p><pre class="screen">
<code class="prompt">[jadebustos@dedalo ~]# </code><strong class="userinput"><code>ipvsadm --list --stats --numeric</code></strong>
<code class="computeroutput">IP Virtual Server version 1.2.0 (size=4096) 
Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes 
  -&gt; RemoteAddress:Port 
TCP  172.16.0.207:443                    7        169     292       x        y 
  -&gt; 172.16.0.199:443                    5        113     214       x        y 
  -&gt; 172.16.0.198:443                    2         56      78       x        y 
TCP  172.16.0.205:9085                   4         67      99       x        y 
  -&gt; 172.16.0.200:9085                   2         34      45       x        y 
  -&gt; 172.16.0.201:9085                   2         33      44       x        y 
TCP  172.16.0.206:80                     7        109     178       x        y 
  -&gt; 172.16.0.197:80                     1         12      23       x        y 
  -&gt; 172.16.0.195:80                     4         67     100       x        y 
  -&gt; 172.16.0.196:80                     2         30      45       x        y 
TCP  172.16.0.207:80                     2         30      58       x        y 
  -&gt; 172.16.0.199:80                     1         13      25       x        y 
  -&gt; 172.16.0.198:80                     1         17      33       x        y 
TCP  172.16.0.205:80                     2         27      50       x        y  
  -&gt; 172.16.0.200:80                     1         15      27       x        y 
  -&gt; 172.16.0.201:80                     1         12      23       x        y </code>
<code class="prompt">[jadebustos@dedalo ~]#</code>
</pre><p>Otras opciones que nos permite:</p><div class="itemizedlist"><ul type="disc"><li><p>Añadir, modificar y borrar servicios.</p></li><li><p>Añadir, modificar y borrar servidores.</p></li><li><p>Modificar los parámetros de configuración de los servicios balanceados.</p></li></ul></div></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2546848"></a>Alta disponibilidad en los balanceadores con <span class="emphasis"><em>keepalived</em></span></h3></div></div></div><p>El balanceador es un punto de fallo crítico. Si el balanceador no está disponible no habrá servicio aunque las máquinas que den el servicio estén en perfecto estado de funcionamiento.</p><div class="mediaobject" align="center"><img src="images/balanceador-2.jpg" align="middle"><div class="caption"><p>Fallo en balanceador.</p></div></div><p>La alta disponibilidad se lográ garantizando que siempre haya un balanceador funcionando, resumiendo el servicio siempre tiene que estar disponible.</p><div class="mediaobject" align="center"><img src="images/balanceador-ha.jpg" align="middle"><div class="caption"><p>Balanceadores en Alta Disponibilidad.</p></div></div><p><a href="http://www.keepalived.org" target="_top">Keepalived</a> es un demonio que se encarga de que siempre haya un balanceador balanceanado el servicio (<span class="emphasis"><em>failover</em></span>). En realidad es un interface a <span class="emphasis"><em>LVS</em></span>.</p><p>Además también se encarga de que no se envíen peticiones a servidores que en ese momento no puedan atenderlas (<span class="emphasis"><em>health-checking</em></span>).</p><p><span class="emphasis"><em>Keepalived</em></span> se instala en los balanceadores. Uno de ellos será el <span class="emphasis"><em>MASTER</em></span> y el resto serán denominados de <span class="emphasis"><em>BACKUP</em></span>. Cuando el <span class="emphasis"><em>MASTER</em></span> deje de estar operativo entrará en funcionamiento el balanceador <span class="emphasis"><em>BACKUP</em></span> de más alta prioridad y continuará balanceando hasta que el <span class="emphasis"><em>MASTER</em></span> vuelva a estar operativo o hasta que tenga algún problema, entonces lo sustituirá otro balanceador de <span class="emphasis"><em>BACKUP</em></span>, el siguiente de más alta prioridad. De esta forma siempre estará balanceando el balanceador de más alta prioridad. En el momento que un balanceador de más alta prioridad que el activo vuelva al servicio asumirá el balanceo.</p><p><span class="emphasis"><em>Keepalived</em></span> utiliza el protocolo <span class="emphasis"><em>VRRP</em></span> (rfc 2338). <span class="emphasis"><em>Keepalived</em></span> formará un balanceador virtual formado por un balanceador <span class="emphasis"><em>MASTER</em></span> y varios balanceadores <span class="emphasis"><em>BACKUP</em></span> funcionando de la manera antés descrita.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>Los balanceadores informarán al resto de su disponibilidad utilizando el protocolo <span class="emphasis"><em>VRRP</em></span>.</p></div><p>Para el <span class="emphasis"><em>health-checking</em></span> de los servicios balanceados <span class="emphasis"><em>keepalived</em></span> puede realizar la comprobación de varias formas:</p><div class="itemizedlist"><ul type="disc"><li><p><span class="emphasis"><em>TCP_CHECK</em></span> se hará una petición TCP y si no es respondida se eliminará el servidor de la lista de servidores activos.</p></li><li><p><span class="emphasis"><em>HTTP_GET</em></span> Se solicitará una página del servidor y se comprobará con una que es la correcta mediante un hashing MD5, previamente calculado,  en caso de no coincidir el servidor será eliminado de la lista de servidores activos.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>Para general el hashing MD5 utilizaremos la utilidad <span><strong class="command">genhash</strong></span> que nos permitirá generar el hashing directamente desde el servidor.</p></div></li><li><p><span class="emphasis"><em>SSL_GET</em></span> igual que <span class="emphasis"><em>HTTP_GET</em></span> pero la página será solicitada bajo una conexión SSL por el puerto 443 (https).</p></li><li><p><span class="emphasis"><em>MISC_CHECK</em></span> esta opción nos permitirá comprobar la disponibilidad de los servidores mediante un script creado por nosotros. Si la situación lo  requiere podremos comprobar la disponibilidad de los servidores de forma personalizada.</p></li></ul></div></div><div class="sect2" lang="es"><div class="titlepage"><div><div><h3 class="title"><a name="id2547116"></a>Configuración de <span class="emphasis"><em>keepalived</em></span></h3></div></div></div><p>El fichero de configuración de <span class="emphasis"><em>keepalived</em></span> normalmente en <code class="filename">/etc/keepalived.conf</code> consta de tres partes:</p><div class="orderedlist"><ol type="1"><li><p><span class="emphasis"><em>Zona de configuración global</em></span> donde se especificará la forma en la que <span class="emphasis"><em>keepalived</em></span> avisará a los administradores de un fallo en el servidor virtual (dirección de correo, servidor SMTP, ...)</p></li><li><p><span class="emphasis"><em>Configuración de VRRP</em></span> donde indicaremos si el balanceador es <span class="emphasis"><em>MASTER</em></span> o <span class="emphasis"><em>BACKUP</em></span>, la prioridad, cual será la interface por la que tiene acceso al servidor virtual, la IP virtual (VIP) ...</p></li><li><p><span class="emphasis"><em>Configuración del servidor virtual</em></span> indicando la IP (VIP) y el puerto, protocolo, ... y una entrada real server con los parámetros de cada servidor real, IP (real), puerto, método de health-checking, ...</p></li></ol></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Importante</h3><p>Este fichero será exactamente igual en el <span class="emphasis"><em>MASTER</em></span> y en los <span class="emphasis"><em>BACKUPS</em></span> salvo que en los <span class="emphasis"><em>BACKUPS</em></span> el parámetro <span class="emphasis"><em>state</em></span> en la configuración de <span class="emphasis"><em>VRRP</em></span> será <span class="emphasis"><em>BACKUP</em></span> y en el <span class="emphasis"><em>MASTER</em></span> será <span class="emphasis"><em>MASTER</em></span> como cabría esperar.</p></div><p>Un ejemplo típico del fichero <code class="filename">/etc/keepalived.conf</code> para una configuración <span class="emphasis"><em>Direct Routing</em></span>:</p><pre class="programlisting">
# Configuration File for keepalived

# Configuracion Global

global_defs {
   notification_email {
     alertas@midominio.com
   }
   notification_email_from balanceador1@midominio.com
   smtp_server 192.168.1.10
   smtp_connect_timeout 30
   lvs_id LVS_DEVEL
}

vrrp_instance wasIntranet {
    state MASTER
    interface eth1
    virtual_router_id 50
    priority 100
    advert_int 1
    wdog-vrrp 1
    virtual_ipaddress {
        172.16.0.205
	172.16.0.206
	172.16.0.207
    }
}

virtual_server 172.16.0.205 80{ 
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.200 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.201 80 {
        weight 1
        TCP_CHECK {
            connect_port 80 
            connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.205 9085 {
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.200 9085 {
        weight 1
        TCP_CHECK {
           connect_port 9085
           connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.201 9085 {
        weight 1
        TCP_CHECK {
           connect_port 9085
           connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.206 80 { 
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.195 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.196 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.197 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.207 80 { 
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.198 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
    real_server 172.16.0.199 80 {
        weight 1
        TCP_CHECK {
            connect_port 80
            connect_timeout 30
        }
        delay_before_retry 3
    }
}

virtual_server 172.16.0.207 443 {
    delay_loop 6
    lb_algo wlc
    lb_kind DR
    persistence_timeout 3600
    protocol TCP

    real_server 172.16.0.198 443 {
         weight 1
         TCP_CHECK {
            connect_port 443
            connect_timeout 30
         }
         delay_before_retry 3
    }
    real_server 172.16.0.199 443 {
         weight 1
         TCP_CHECK {
             connect_port 443
             connect_timeout 30
         }
         delay_before_retry 3
    }
}
</pre></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ch03s03.html">Anterior</a> </td><td width="20%" align="center"><a accesskey="u" href="ch03.html">Subir</a></td><td width="40%" align="right"> <a accesskey="n" href="ch04.html">Siguiente</a></td></tr><tr><td width="40%" align="left" valign="top">Balanceo en DNS </td><td width="20%" align="center"><a accesskey="h" href="index.html">Inicio</a></td><td width="40%" align="right" valign="top"> Capítulo 4. Detección de fallos en los nodos del cluster.</td></tr></table></div></body></html>
